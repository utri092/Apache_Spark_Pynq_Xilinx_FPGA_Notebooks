{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Welcome to PYNQ\n",
    "\n",
    "## Getting Started\n",
    "\n",
    "To get started using PYNQ, try running the example notebooks in the folders described below. \n",
    "\n",
    "* **getting_started**: includes an introduction to using Jupyter notebook with PYNQ, the Python environment, and how to use some basic features of the curernt platform. \n",
    "\n",
    "* **common**: contains example notebooks on how to download an overlay, how to set the Zynq clocks, how to execute Linux shell commands, and how to use USB devices.\n",
    "\n",
    "If other overlays or packages are installed, other folders with example notebooks may also be available in this directory.  \n",
    "\n",
    "\n",
    "## Documentation\n",
    "\n",
    "Please see the latest <a href=\"http://pynq.readthedocs.io\">PYNQ Documentation on readthedocs</a>.  \n",
    "\n",
    "\n",
    "## Support\n",
    "\n",
    "For questions or support, go to the forum on the <a href=\"http://www.pynq.io\">PYNQ project webpage </a>.\n",
    "\n",
    "\n",
    "## Project webpage\n",
    "\n",
    "You can find details on the <a href=\"http://www.pynq.io\">PYNQ project webpage </a>.\n",
    "\n",
    "\n",
    "## GitHub\n",
    "\n",
    "The PYNQ Repository is hosted on github: <a href=\"https://github.com/Xilinx/PYNQ\">PYNQ GitHub Repository</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.5'"
      ]
     },
     "execution_count": 280,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyspark.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config('spark.sql.execution.arrow.fallback.enabled', 'true')\\\n",
    "    .config(\"spark.sql.execution.arrow.enabled\", \"true\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+---+\n",
      "|       _c0|_c1|_c2|\n",
      "+----------+---+---+\n",
      "|1582999200|  1|109|\n",
      "|1582999260|  1|100|\n",
      "|1582999320|  1|104|\n",
      "|1582999380|  1|108|\n",
      "|1582999440|  1|105|\n",
      "|1582999500|  1| 99|\n",
      "|1582999560|  1| 96|\n",
      "|1582999620|  1|102|\n",
      "|1582999680|  1|121|\n",
      "|1582999740|  1| 99|\n",
      "|1582999800|  1|109|\n",
      "|1582999860|  1| 93|\n",
      "|1582999920|  1|113|\n",
      "|1582999980|  1|106|\n",
      "|1583000040|  1|112|\n",
      "|1583000100|  1|101|\n",
      "|1583000160|  1|108|\n",
      "|1583000220|  1|110|\n",
      "|1583000280|  1|108|\n",
      "|1583000340|  1|104|\n",
      "+----------+---+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.csv('hdfs://afog-master:9000/part4-projects/resources/benchmarks/dataset-1_converted.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|       _c0|_c1|\n",
      "+----------+---+\n",
      "|1582999200|  1|\n",
      "|1582999260|  1|\n",
      "|1582999320|  1|\n",
      "|1582999380|  1|\n",
      "|1582999440|  1|\n",
      "|1582999500|  1|\n",
      "|1582999560|  1|\n",
      "|1582999620|  1|\n",
      "|1582999680|  1|\n",
      "|1582999740|  1|\n",
      "|1582999800|  1|\n",
      "|1582999860|  1|\n",
      "|1582999920|  1|\n",
      "|1582999980|  1|\n",
      "|1583000040|  1|\n",
      "|1583000100|  1|\n",
      "|1583000160|  1|\n",
      "|1583000220|  1|\n",
      "|1583000280|  1|\n",
      "|1583000340|  1|\n",
      "+----------+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get rid of slotOccupancy\n",
    "df = df.drop(df.columns[2])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/xilinx/jupyter_notebooks'"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU:   650.000000MHz\n",
      "FCLK0: 76.923077MHz\n"
     ]
    }
   ],
   "source": [
    "from pynq import Overlay, Clocks\n",
    "\n",
    "print(f'CPU:   {Clocks.cpu_mhz:.6f}MHz')\n",
    "print(f'FCLK0: {Clocks.fclk0_mhz:.6f}MHz')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use bitstream for PL \n",
    "overlay = Overlay(os.getcwd() + \"/smart_parking_stream/dpu_ip/dpu.bit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "overlay?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPU IP\n",
    "dpu_ip = overlay.dpu_ip\n",
    "dpu_ip?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DMA IP\n",
    "dma = overlay.axi_dma\n",
    "dma?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "hw_timer = overlay.axi_timer\n",
    "hw_timer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegisterMap {\n",
       "  CTRL = Register(AP_START=0, AP_DONE=0, AP_IDLE=1, AP_READY=0, RESERVED_1=0, AUTO_RESTART=0, RESERVED_2=0),\n",
       "  GIER = Register(Enable=0, RESERVED=0),\n",
       "  IP_IER = Register(CHAN0_INT_EN=0, CHAN1_INT_EN=0, RESERVED=0),\n",
       "  IP_ISR = Register(CHAN0_INT_ST=0, CHAN1_INT_ST=0, RESERVED=0),\n",
       "  max_size = Register(max_size=0),\n",
       "  max_size_ctrl = Register(max_size_ap_vld=0, RESERVED=0)\n",
       "}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  Shows the Registers we need to access - Can use access via names or direct memory  (via names is easier)\n",
    "dpu_ip.register_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Smart Parking Stream\n",
    "\n",
    "# First signal to set high. Ensures AP_START Signal does not go low after one cycle\n",
    "# In AXI_STREAM, only setting AP_START enables computations for 1 stream\n",
    "dpu_ip.register_map.CTRL.AUTO_RESTART = 1\n",
    "# Computations occur while high\n",
    "dpu_ip.register_map.CTRL.AP_START = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RegisterMap {\n",
       "  CTRL = Register(AP_START=1, AP_DONE=1, AP_IDLE=0, AP_READY=1, RESERVED_1=0, AUTO_RESTART=1, RESERVED_2=0),\n",
       "  GIER = Register(Enable=0, RESERVED=0),\n",
       "  IP_IER = Register(CHAN0_INT_EN=0, CHAN1_INT_EN=0, RESERVED=0),\n",
       "  IP_ISR = Register(CHAN0_INT_ST=0, CHAN1_INT_ST=0, RESERVED=0),\n",
       "  max_size = Register(max_size=4294967295),\n",
       "  max_size_ctrl = Register(max_size_ap_vld=1, RESERVED=0)\n",
       "}"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if signals were set and read only registers have values\n",
    "dpu_ip.register_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4294967295\n"
     ]
    }
   ],
   "source": [
    "# Maximum no of elements dpu accepts in input Stream\n",
    "## This is for a finite size for loop implementation\n",
    "## While loop implementation can accept unlimited size of stream\n",
    "max_size = int(dpu_ip.register_map.max_size)\n",
    "print(max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "################# Test: Feed a single stream ##################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyarrow\r\n",
      "Version: 0.17.0\r\n",
      "Summary: Python library for Apache Arrow\r\n",
      "Home-page: https://arrow.apache.org/\r\n",
      "Author: Apache Arrow Developers\r\n",
      "Author-email: dev@arrow.apache.org\r\n",
      "License: Apache License, Version 2.0\r\n",
      "Location: /usr/local/lib/python3.6/dist-packages\r\n",
      "Requires: numpy\r\n",
      "Required-by: \r\n"
     ]
    }
   ],
   "source": [
    "# Does near zero time Spark to Pandas df conversion. \n",
    "# Required features not supported on Armv7 as of yet\n",
    "# @link for issue I put up \n",
    "#       https://issues.apache.org/jira/browse/ARROW-10276\n",
    "!pip show pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/spark/python/pyspark/sql/dataframe.py:2110: UserWarning: toPandas attempted Arrow optimization because 'spark.sql.execution.arrow.enabled' is set to true; however, failed by the reason below:\n",
      "  module 'pyarrow' has no attribute 'compat'\n",
      "Attempting non-optimization as 'spark.sql.execution.arrow.fallback.enabled' is set to true.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "df_pd = df.toPandas()\n",
    "npArr = df_pd.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.tail of                _c0 _c1\n",
       "0       1582999200   1\n",
       "1       1582999260   1\n",
       "2       1582999320   1\n",
       "3       1582999380   1\n",
       "4       1582999440   1\n",
       "...            ...  ..\n",
       "172404  1590628995   1\n",
       "172405  1590629055   1\n",
       "172406  1590629115   1\n",
       "172407  1590629175   1\n",
       "172408  1590629235   1\n",
       "\n",
       "[172409 rows x 2 columns]>"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pd.tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(172409, 2)\n",
      "(344818,)\n"
     ]
    }
   ],
   "source": [
    "# Convert 2D to 1D array\n",
    "print(npArr.shape)\n",
    "npArr = npArr.flatten()\n",
    "print(npArr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1582999200          1 1582999260 ...          1 1590629235          1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from pynq import allocate\n",
    "\n",
    "row_length = 2\n",
    "\n",
    "inStream = allocate(shape=(npArr.size), dtype=np.uint32)\n",
    "\n",
    "# Simply assigning the list will not work\n",
    "for i, x in enumerate(npArr):\n",
    "    inStream[i] = npArr[i]\n",
    "\n",
    "# Confirm records were transferred\n",
    "print(inStream)\n",
    "\n",
    "outStream = allocate(shape=(npArr.size//row_length), dtype=np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed transfer inStream\n"
     ]
    }
   ],
   "source": [
    "dma.sendchannel.transfer(inStream)\n",
    "print(\"Completed transfer inStream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed transfer outStream\n"
     ]
    }
   ],
   "source": [
    "dma.recvchannel.transfer(outStream)\n",
    "print(\"Completed transfer outStream\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed send Channel wait\n"
     ]
    }
   ],
   "source": [
    "# Call if TLAST is not set high in HLS code\n",
    "dma.sendchannel.wait()\n",
    "print(\"Completed send Channel wait\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed recv Channel wait\n"
     ]
    }
   ],
   "source": [
    "# Call if TLAST is not set high in HLS code\n",
    "dma.recvchannel.wait()\n",
    "print(\"Completed recv Channel wait\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[104 105 105 ... 105 105 104]\n"
     ]
    }
   ],
   "source": [
    "# outStream.flush()\n",
    "print(outStream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freed inStream buffer\n"
     ]
    }
   ],
   "source": [
    "inStream.freebuffer()\n",
    "print(\"Freed inStream buffer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Freed outStream buffer\n"
     ]
    }
   ],
   "source": [
    "outStream.freebuffer()\n",
    "print(\"Freed outStream buffer\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
